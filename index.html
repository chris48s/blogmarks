<!DOCTYPE html>
<html prefix="        og: http://ogp.me/ns# article: http://ogp.me/ns/article#     " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Handy links or snippets, and a sentence or two">
<meta name="viewport" content="width=device-width">
<title>Blogmarks</title>
<link href="assets/css/baguetteBox.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link href="assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="assets/css/custom.css" rel="stylesheet" type="text/css">
<link href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="rss.xml">
<link rel="canonical" href="https://chris48s.github.io/blogmarks/">
<link rel="next" href="index-6.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5shiv-printshiv.min.js"></script><![endif]--><link rel="prefetch" href="posts/2026/cherry-pick-range/" type="text/html">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
        
    <header id="header"><h1 id="brand"><a href="." title="Blogmarks" rel="home">

        <span id="blog-title">Blogmarks</span>
    </a></h1>

        

        
    <nav id="menu"><ul>
<li><a href="rss.xml">RSS feed</a></li>
                <li><a href="categories/">Tags</a></li>

    

    
    
    </ul></nav></header><main id="content"><div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2026/cherry-pick-range/" class="u-url">Cherry-picking a range of commits in git</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2026/cherry-pick-range/" rel="bookmark">
            <time class="published dt-published" datetime="2026-01-27T00:00:00Z" itemprop="datePublished" title="2026-01-27">2026-01-27</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Sometimes it is useful to cherry pick multiple commits in a single operation.</p>
<p>There most frequent way I do this is by cherry-picking a range using the syntax</p>
<div class="code"><pre class="code literal-block">git cherry-pick abc123^..def456
</pre></div>

<p>This applies that range of commits in order. The <code>^</code> is important because git ranges are exclusive on the left and inclusive on the right.</p>
<ul>
<li>
<code>abc123..def456</code> applies commits <strong>after</strong> <code>abc123</code>, up to and including <code>def456</code>
</li>
<li>
<code>abc123^..def456</code> applies commits starting with <code>abc123</code>, up to and including <code>def456</code>
</li>
</ul>
<p>The caret tells git to include the first commit.</p>
<p>I usually check that</p>
<div class="code"><pre class="code literal-block">git log --oneline abc123^..def456
</pre></div>

<p>shows the list of commits I expect before running the cherry-pick.</p>
<p>I use this one less often, but it is also possible to cherry-pick multiple individual commits in one command. For example:</p>
<div class="code"><pre class="code literal-block">git cherry-pick 7f3a9c2 b81e4d6 d4e8b73
</pre></div>

<p>Git applies the commits in the order listed, left to right. This is useful when you want to pick a non-continuous range or you want to re-order while picking.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2025/throwing-warnings/" class="u-url">Throwing warnings as if they are exceptions</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2025/throwing-warnings/" rel="bookmark">
            <time class="published dt-published" datetime="2025-11-05T00:00:00Z" itemprop="datePublished" title="2025-11-05">2025-11-05</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Sometimes a warning is logged from third-party code in a way that doesn't make it clear where the problem is in your own code. For example, today I was trying to track down the cause of this:</p>
<div class="code"><pre class="code literal-block">.venv/lib/python3.12/site-packages/django/template/base.py:1047: RemovedInDjango50Warning: The "default.html" templates for forms and formsets will be removed. These were proxies to the equivalent "table.html" templates, but the new "div.html" templates will be the default from Django 5.0. Transitional renderers are provided to allow you to opt-in to the new output style now. See https://docs.djangoproject.com/en/4.2/releases/4.1/ for more details
</pre></div>

<p>I understand the message, but <code>.venv/lib/python3.12/site-packages/django/template/base.py</code> doesn't tell me where in my own codebase to look for the problem.</p>
<p>Today I learned that you can tell pytest (and indeed, python itself) to throw warnings as if they are an exception. So you can run</p>
<div class="code"><pre class="code literal-block">pytest<span class="w"> </span>-W<span class="w"> </span>error
</pre></div>

<p>to treat all warnings as if they are an exception, or something like</p>
<div class="code"><pre class="code literal-block">pytest<span class="w"> </span>-W<span class="w"> </span>error::django.utils.deprecation.RemovedInDjango50Warning
</pre></div>

<p>to only apply this behaviour to a particular class of warning.</p>
<p>Either way, this will give you a stacktrace from the point where the warning is logged which can give you some breadcrumbs to follow.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2025/masto-host-behind-cloudflare/" class="u-url">Running masto.host behind CloudFlare</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2025/masto-host-behind-cloudflare/" rel="bookmark">
            <time class="published dt-published" datetime="2025-08-17T00:00:00+01:00" itemprop="datePublished" title="2025-08-17">2025-08-17</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>My mastodon instance is managed by <a href="https://masto.host/">masto.host</a> and I use CloudFlare for DNS. If you use this combination of services and run "Check DNS" in the masto.host dashboard, it will tell you</p>
<blockquote>
<p>NOTICE: CloudFlare Proxy needs to be disabled.
The Proxy Status for that record should read 'DNS Only'.
If you see the 'orange cloud' next to the record then Proxy is enabled and you need to disable it.</p>
</blockquote>
<p>This is a working configuration, but running in DNS Only mode means there is no cache/CDN in front of your instance. It also means a lot of useful CloudFlare features are not available to you as they only work when CloudFlare is acting as a proxy.</p>
<p>Although masto.host tells you to set CloudFlare to DNS Only, they do this to reduce support overhead for CloudFlare users. That is understandable, but it is perfectly possible to run CloudFlare in proxy mode. In my case, I needed to do two things:</p>
<ol>
<li>Ensure the TLS mode is set to <a href="https://developers.cloudflare.com/ssl/origin-configuration/ssl-modes/full-strict/">Full (strict)</a>. I want this anyway because my origin has its own TLS cert, but Flexible mode will cause a <a href="https://developers.cloudflare.com/ssl/troubleshooting/too-many-redirects/">redirect loop</a> because the upstream server redirects http:// requests to https://.</li>
<li>Turn off <a href="https://developers.cloudflare.com/ssl/edge-certificates/additional-options/always-use-https/">Always Use HTTPS</a>. This is needed so that the origin's TLS certificate can be <a href="https://community.cloudflare.com/t/let-lets-encrypt-bot-bypass-always-use-https/200274">renewed by Let's Encrypt Bot</a>.</li>
</ol>
</div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2025/django-db-routers-2/" class="u-url">Django DB Routers: Avoiding Race Conditions</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2025/django-db-routers-2/" rel="bookmark">
            <time class="published dt-published" datetime="2025-04-04T00:00:00+01:00" itemprop="datePublished" title="2025-04-04">2025-04-04</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>If you are using django with a Postgres primary/replica setup for the first time, your first instinct may be to write something like this:</p>
<div class="code"><pre class="code literal-block"><span class="n">DATABASES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"default"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"ENGINE"</span><span class="p">:</span> <span class="s2">"django.db.backends.postgresql"</span><span class="p">,</span>
        <span class="o">...</span>
    <span class="p">},</span>
    <span class="s2">"replica1"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"ENGINE"</span><span class="p">:</span> <span class="s2">"django.db.backends.postgresql"</span><span class="p">,</span>
        <span class="o">...</span>
    <span class="p">},</span>
    <span class="s2">"replica2"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"ENGINE"</span><span class="p">:</span> <span class="s2">"django.db.backends.postgresql"</span><span class="p">,</span>
        <span class="o">...</span>
    <span class="p">},</span>
<span class="p">}</span>


<span class="kn">import</span> <span class="nn">random</span>

<span class="k">class</span> <span class="nc">AuthRouter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">db_for_read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">hints</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="p">[</span><span class="s2">"replica1"</span><span class="p">,</span> <span class="s2">"replica2"</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">db_for_write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">hints</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">"default"</span>
    <span class="o">...</span>
</pre></div>

<p>Read from the replicas, write to the primary - simple. There are tutorials online that will tell you to do this.</p>
<p>However, there's a problem with this setup: You now have a race condition. Replication is not instant. If you write something to the primary and then read back the same record from one of the replicas, the data may be stale.</p>
<p>How you choose to solve this depends on your application, but here's one strategy I have recently used.</p>
<p>A site I am working on has essentially a read-only public-facing site with a large number of users. Writes are made either by a handful of internal users via django's admin backend, or by management commands. We want to prevent race conditions in contexts where we will perform writes and scale DB reads in the public-facing parts of the site.</p>
<p>In order to do this, we used a package called <a href="https://pypi.org/project/django-middleware-global-request/">django-middleware-global-request</a>. This package provides a middleware that can store the current request in a thread-local variable so it can be accessed in contexts where a request would not normally be in scope.</p>
<p>Using this package allows us to write the following code:</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">django_middleware_global_request</span> <span class="kn">import</span> <span class="n">get_request</span>

<span class="k">class</span> <span class="nc">AuthRouter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">db_for_read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">hints</span><span class="p">):</span>
        <span class="n">request</span> <span class="o">=</span> <span class="n">get_request</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">request</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">request</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"/admin"</span><span class="p">):</span>
                <span class="c1"># always read from the primary</span>
                <span class="c1"># in the admin interface</span>
                <span class="k">return</span> <span class="s2">"default"</span>

            <span class="c1"># use replicas to serve read traffic</span>
            <span class="c1"># on the public site</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
                <span class="p">[</span><span class="s2">"replica1"</span><span class="p">,</span> <span class="s2">"replica2"</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># if there's no request in scope,</span>
        <span class="c1"># read from the primary</span>
        <span class="c1"># we don't care about trying to "scale"</span>
        <span class="c1"># management commands</span>
        <span class="k">return</span> <span class="s2">"default"</span>

    <span class="k">def</span> <span class="nf">db_for_write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">hints</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">"default"</span>
    <span class="o">...</span>
</pre></div>

<p>There are some situations where this wouldn't work. Obviously if you can't generalise about traffic based on the URL this strategy isn't going to hold water. Perhaps less obviously, if you've got an application which uses a lot of background tasks (which also will not have a request in scope) you might want to also serve some reads from the replicas in that situation. This approach would serve all the reads from the primary in that context.</p>
<p>For my use-case, this worked like a charm.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2025/django-db-routers-1/" class="u-url">Django DB Routers: Pitfalls</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2025/django-db-routers-1/" rel="bookmark">
            <time class="published dt-published" datetime="2025-02-16T00:00:00Z" itemprop="datePublished" title="2025-02-16">2025-02-16</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Django has a feature called <a href="https://docs.djangoproject.com/en/5.2/topics/db/multi-db/">Database Routers</a>. This gives you an application-level abstraction to manage multiple database connections. This can be useful if you are working with a primary/replica setup or if you're implementing database partitioning or sharding in your application.</p>
<p>There are a few ways in which it is can be a bit of a leaky abstraction though.</p>
<h3>Raw SQL</h3>
<p>This one is pretty obvious if you think about it, but anything you define in your database routers only applies to database I/O that happens via the ORM. As soon as you use a <code>cursor</code> to execute raw SQL your DB routers are bypassed and it is your responsibility to manage which connection is being used. You can do this by importing <code>from django.db import connections</code> and selecting a named connection.</p>
<p>This isn't a big surprise, but it is worth being aware of if you are converting an existing application to use multiple DB connections. This is not normally something you implement from day one.</p>
<p>The other thing worth noting here is that even if your own application code only interacts with the DB through the ORM, if there is any package anywhere in your dependency tree that uses <code>connection.cursor()</code>, that is going to bypass any logic in your database router. This could lead to reads or writes performed by a third party package which don't happen on the expected DB connection.</p>
<h3>Transactions</h3>
<p>This one is perhaps less obvious. Any time you use a transaction, you are also responsible for explicitly managing your own DB connections. Imagine we have an application with the following setup:</p>
<div class="code"><pre class="code literal-block"><span class="n">DATABASES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"default"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"ENGINE"</span><span class="p">:</span> <span class="s2">"django.db.backends.sqlite3"</span><span class="p">,</span>
        <span class="s2">"NAME"</span><span class="p">:</span> <span class="n">BASE_DIR</span> <span class="o">/</span> <span class="s2">"db1.sqlite3"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">"auth"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"ENGINE"</span><span class="p">:</span> <span class="s2">"django.db.backends.sqlite3"</span><span class="p">,</span>
        <span class="s2">"NAME"</span><span class="p">:</span> <span class="n">BASE_DIR</span> <span class="o">/</span> <span class="s2">"db2.sqlite3"</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>

<span class="k">class</span> <span class="nc">AuthRouter</span><span class="p">:</span>
    <span class="n">route_app_labels</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"auth"</span><span class="p">,</span> <span class="s2">"contenttypes"</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">db_for_read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">hints</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">_meta</span><span class="o">.</span><span class="n">app_label</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">route_app_labels</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">"auth"</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">db_for_write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">hints</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">_meta</span><span class="o">.</span><span class="n">app_label</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">route_app_labels</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">"auth"</span>
        <span class="k">return</span> <span class="kc">None</span>
</pre></div>

<p>If I write the following code:</p>
<div class="code"><pre class="code literal-block"><span class="k">with</span> <span class="n">transaction</span><span class="o">.</span><span class="n">atomic</span><span class="p">():</span>
    <span class="n">User</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">create_user</span><span class="p">(</span>
        <span class="n">username</span><span class="o">=</span><span class="s2">"user1"</span><span class="p">,</span>
        <span class="n">email</span><span class="o">=</span><span class="s2">"user1@example.com"</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="s2">"changeme1"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">User</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">create_user</span><span class="p">(</span>
        <span class="n">username</span><span class="o">=</span><span class="s2">"user2"</span><span class="p">,</span>
        <span class="n">email</span><span class="o">=</span><span class="s2">"user2@example.com"</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="s2">"changeme2"</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>

<p>this will create 2 users, writing the records to our <code>auth</code> DB. But the transaction wrapping those writes uses the <code>default</code> DB connection. This won't throw any exception or error. It just means these two writes are not wrapped in a meaningful transaction. However, to a casual reader of the code, it probably looks like they are.</p>
<p>The correct code in this case would be <code>with transaction.atomic(using="auth"):</code> but it is worth being aware of this. Using multiple DBs opens the door to a class of wrong code that looks right.</p>
<h3>Data Migrations</h3>
<p>It is also important to be careful when writing custom migrations using <code>RunPython</code>. Continuing to use our example above where we have a <code>default</code> DB and an <code>auth</code> DB, consider the following migration code:</p>
<div class="code"><pre class="code literal-block"><span class="k">def</span> <span class="nf">create_user</span><span class="p">(</span><span class="n">apps</span><span class="p">,</span> <span class="n">schema_editor</span><span class="p">):</span>
    <span class="n">User</span> <span class="o">=</span> <span class="n">apps</span><span class="o">.</span><span class="n">get_model</span><span class="p">(</span><span class="s2">"auth"</span><span class="p">,</span> <span class="s2">"User"</span><span class="p">)</span>
    <span class="n">User</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">create_user</span><span class="p">(</span>
        <span class="n">username</span><span class="o">=</span><span class="s2">"user1"</span><span class="p">,</span>
        <span class="n">email</span><span class="o">=</span><span class="s2">"user1@example.com"</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="s2">"changeme1"</span><span class="p">,</span>
    <span class="p">)</span>

<span class="k">class</span> <span class="nc">Migration</span><span class="p">(</span><span class="n">migrations</span><span class="o">.</span><span class="n">Migration</span><span class="p">):</span>
    <span class="n">operations</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">migrations</span><span class="o">.</span><span class="n">RunPython</span><span class="p">(</span>
            <span class="n">create_user</span><span class="p">,</span>
            <span class="n">reverse_code</span><span class="o">=</span><span class="n">migrations</span><span class="o">.</span><span class="n">RunPython</span><span class="o">.</span><span class="n">noop</span>
        <span class="p">)</span>
    <span class="p">]</span>
</pre></div>

<p>This code looks fairly reasonable on the surface. If you are moving to using multiple DBs in an application which has historically run on a single DB, you probably have some migrations like this kicking about in the project history. The catch here is: If we run <code>./manage.py migrate</code> it will attempt to create the User object on the <code>default</code> DB connection. That will also happen if we run <code>./manage.py migrate --database auth</code>. This happens because the model retrieved via <code>apps.get_model()</code> is unaware of the database being used in the migration process. Django uses the default database unless explicitly specified.</p>
<p>The correct code in this case would be:</p>
<div class="code"><pre class="code literal-block"><span class="k">def</span> <span class="nf">create_user</span><span class="p">(</span><span class="n">apps</span><span class="p">,</span> <span class="n">schema_editor</span><span class="p">):</span>
    <span class="n">User</span> <span class="o">=</span> <span class="n">apps</span><span class="o">.</span><span class="n">get_model</span><span class="p">(</span><span class="s2">"auth"</span><span class="p">,</span> <span class="s2">"User"</span><span class="p">)</span>
    <span class="n">User</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">using</span><span class="p">(</span>
        <span class="n">schema_editor</span><span class="o">.</span><span class="n">connection</span><span class="o">.</span><span class="n">alias</span>
    <span class="p">)</span><span class="o">.</span><span class="n">create_user</span><span class="p">(</span>
        <span class="n">username</span><span class="o">=</span><span class="s2">"user1"</span><span class="p">,</span>
        <span class="n">email</span><span class="o">=</span><span class="s2">"user1@example.com"</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="s2">"changeme1"</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2025/moto-lambda-logs/" class="u-url">Three lessons from moving to Renovate</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2025/moto-lambda-logs/" rel="bookmark">
            <time class="published dt-published" datetime="2025-01-19T00:00:00Z" itemprop="datePublished" title="2025-01-19">2025-01-19</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>At work recently I have been migrating some python projects using UV for package management from dependabot to renovate. Renovate is really impressive software. It can do <em>all the things</em>. However with that capability comes some complexity. Whereas dependabot is a go kart, renovate is more like an aeroplane cockpit. Although I've used renovate before, the projects I've been migrating recently are a bit bigger with a few more moving parts.</p>
<p>Here's a random grab-bag of things I have recently learned that were not immediately obvious to me.</p>
<h3>Order of packageRules evaluation</h3>
<p>Renovate evaluates <code>packageRules[]</code> from top to bottom. The order they are declared in is important. If a package matches a rule, thereâ€™s no bail-out or "early return". It keeps going down the array. This means if a package matches multiple rules, later rules override earlier rules in the array.</p>
<p>To give a worked example:</p>
<p>If I have</p>
<div class="code"><pre class="code literal-block"><span class="k">[project]</span>
<span class="n">dependencies</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s">"boto3==1.35.98"</span><span class="p">,</span>
<span class="w">    </span><span class="s">"botocore==1.35.98"</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>

<p>In my <code>pyproject.toml</code></p>
<p>This renovate config</p>
<div class="code"><pre class="code literal-block"><span class="nt">"packageRules"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"matchManagers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"pep621"</span><span class="p">],</span>
<span class="w">        </span><span class="nt">"matchDepTypes"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"project.dependencies"</span><span class="p">],</span>
<span class="w">        </span><span class="nt">"schedule"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"before 4am on monday"</span><span class="p">],</span><span class="w"> </span><span class="c1">// weekly</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"groupName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"boto"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"matchManagers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"pep621"</span><span class="p">],</span>
<span class="w">        </span><span class="nt">"matchPackageNames"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"/^boto.*$/"</span><span class="p">],</span>
<span class="w">        </span><span class="nt">"schedule"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"* 0-3 1 * *"</span><span class="p">],</span><span class="w"> </span><span class="c1">// monthly</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">]</span>
</pre></div>

<p>Will bump the two boto packages <strong>monthly</strong>, whereas the same rules in the opposite order</p>
<div class="code"><pre class="code literal-block"><span class="nt">"packageRules"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"groupName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"boto"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"matchManagers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"pep621"</span><span class="p">],</span>
<span class="w">        </span><span class="nt">"matchPackageNames"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"/^boto.*$/"</span><span class="p">],</span>
<span class="w">        </span><span class="nt">"schedule"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"* 0-3 1 * *"</span><span class="p">],</span><span class="w"> </span><span class="c1">// monthly</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"matchManagers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"pep621"</span><span class="p">],</span>
<span class="w">        </span><span class="nt">"matchDepTypes"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"project.dependencies"</span><span class="p">],</span>
<span class="w">        </span><span class="nt">"schedule"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"before 4am on monday"</span><span class="p">],</span><span class="w"> </span><span class="c1">// weekly</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">]</span>
</pre></div>

<p>will bump them <strong>weekly</strong> because the packages match both rules and the second one "wins".</p>
<p>This means that you broadly want to order your <code>packageRules</code> from the most general at the start of the array to most specific at the end.</p>
<h3>VCS Dependencies</h3>
<p>UV allows you to specify VCS dependencies in a couple of ways.</p>
<p>One of them is to do it inline in <code>dependencies</code> using PEP-508 syntax e.g:</p>
<div class="code"><pre class="code literal-block"><span class="k">[project]</span>
<span class="n">dependencies</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s">"arcgis2geojson @ git+https://github.com/chris48s/arcgis2geojson.git@3.0.3"</span>
<span class="p">]</span>
</pre></div>

<p>The other us to use UV <a href="https://docs.astral.sh/uv/concepts/projects/dependencies/#dependency-sources">dependency sources</a> e.g:</p>
<div class="code"><pre class="code literal-block"><span class="k">[project]</span>
<span class="n">dependencies</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s">"arcgis2geojson"</span>
<span class="p">]</span>

<span class="k">[tool.uv.sources]</span>
<span class="n">arcgis2geojson</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">git</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">"https://github.com/chris48s/arcgis2geojson.git"</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">"3.0.3"</span><span class="w"> </span><span class="p">}</span>
</pre></div>

<p>Renovate's UV manager natively knows how to bump VCS dependencies in <code>pyproject.toml</code>. However, it will only detect and bump dependencies which are specified with the dependency sources syntax. VCS dependencies specified inline will be ignored, so the syntax you use here matters.</p>
<p>Additionally, the specific syntax you use inside <code>tool.uv.sources</code> is also important. These two declarations are essentially identical in terms of what gets installed into your virtual environment:</p>
<div class="code"><pre class="code literal-block"><span class="k">[tool.uv.sources]</span>
<span class="n">arcgis2geojson</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">git</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">"https://github.com/chris48s/arcgis2geojson.git"</span><span class="p">,</span><span class="w"> </span><span class="n">rev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">"3.0.3"</span><span class="w"> </span><span class="p">}</span>
</pre></div>

<div class="code"><pre class="code literal-block"><span class="k">[tool.uv.sources]</span>
<span class="n">arcgis2geojson</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">git</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">"https://github.com/chris48s/arcgis2geojson.git"</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">"3.0.3"</span><span class="w"> </span><span class="p">}</span>
</pre></div>

<p>However, renovate will interpret them and bump them differently. Using <code>rev</code> here will cause renovate to bump you to the latest commit every time a commit is pushed to the referenced repository, whereas using <code>tag</code> will only offer to upgrade the dependency when a new tag is pushed.</p>
<h3>Pre-Commit</h3>
<p>I'm going to go on record and say I don't particularly like pre-commit. It is not something I use on my own projects. I do encounter on other people's repositories a lot though, including some of the repos at work. As such, although it is something I prefer not to use, I do need to find coping mechanisms for it. One of my least favourite things about pre-commit is that unless you use repository-local hooks (which is relatively uncommon), pre-commit pushes you towards using custom "hook" repos. This leads to a situation where you end up with the version numbers of your tools duplicated in your package manifest and your <code>.pre-commit.yaml</code>. You now no longer have a single source of truth for this information and the two version numbers can end up out of sync.</p>
<p>Usefully, renovate has a <a href="https://docs.renovatebot.com/modules/manager/pre-commit/">pre-commit manager</a> which can bump version numbers in <code>.pre-commit.yaml</code>. This means you can write a <code>packageRule</code> like:</p>
<div class="code"><pre class="code literal-block"><span class="p">{</span>
<span class="w">    </span><span class="nt">"groupName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ruff"</span><span class="p">,</span>
<span class="w">    </span><span class="nt">"matchManagers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">"pep621"</span><span class="p">,</span>
<span class="w">        </span><span class="s2">"pre-commit"</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">"matchPackageNames"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">"ruff"</span><span class="p">,</span>
<span class="w">        </span><span class="s2">"astral-sh/ruff-pre-commit"</span>
<span class="w">    </span><span class="p">],</span>
<span class="p">}</span>
</pre></div>

<p>for example, which will bump the version of ruff in <code>pyproject.toml</code>/<code>uv.lock</code> and <code>.pre-commit.yaml</code> in a single PR, keeping the two in sync.</p>
<p>Note that renovate's pre-commit manager is <strong>disabled by default</strong> and you must explicitly opt-in to it, as noted in the <a href="https://docs.renovatebot.com/modules/manager/pre-commit/#enabling">docs</a>.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2024/moto-lambda-logs/" class="u-url">Where do the logs from fake lambda go?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2024/moto-lambda-logs/" rel="bookmark">
            <time class="published dt-published" datetime="2024-10-30T00:00:00Z" itemprop="datePublished" title="2024-10-30">2024-10-30</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I've written before about <a href="https://docs.getmoto.org/en/latest/">moto</a>. It is a library for mocking out AWS services under test, and it is basically magic.</p>
<p>Recently, I was working on a test for some code that invokes an AWS lambda function, and I was using moto to mock out the lambda service. The fact that moto can do this is quite impressive in itself, but during the course of this I found myself needing some visibility onto what was happening in the lambda while moto was running it in the mock lambda environment. This led me to the question "Where do the logs from fake lambda go?"</p>
<p>It turns out the answer to this question is: "Into fake CloudWatch".</p>
<p>On one level this answer makes complete logical sense. Simultaneously I found this a bit mind-blowing ðŸ¤¯ Moto really is incredibly capable software.</p>
<p>So, armed with this knowledge, how do we use it?</p>
<p>I've pushed a repo with some complete working code demonstrating this to <a href="https://github.com/chris48s/moto-lambda-logs-demo">https://github.com/chris48s/moto-lambda-logs-demo</a>. Here's a cut down version leaving out some imports and helper functions for the sake of brevity.</p>
<p>First lets define a toy lambda function we can test. As well as returning a response, our handler also prints something to stdout.</p>
<div class="code"><pre class="code literal-block"><span class="c1"># handler.py</span>
<span class="k">def</span> <span class="nf">lambda_handler</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"log message"</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">"statusCode"</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span> <span class="s2">"body"</span><span class="p">:</span> <span class="s2">"Hello from Lambda!"</span><span class="p">}</span>
</pre></div>

<p>We can use moto to run this handler in a mock lambda environment and test the response like this:</p>
<div class="code"><pre class="code literal-block"><span class="k">def</span> <span class="nf">test_lambda</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">mock_aws</span><span class="p">():</span>
        <span class="n">role</span> <span class="o">=</span> <span class="n">_get_mock_role</span><span class="p">()</span>

        <span class="n">lambda_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span>
            <span class="s2">"lambda"</span><span class="p">,</span>
            <span class="n">region_name</span><span class="o">=</span><span class="s2">"eu-west-1"</span>
        <span class="p">)</span>

        <span class="n">fn</span> <span class="o">=</span> <span class="n">lambda_client</span><span class="o">.</span><span class="n">create_function</span><span class="p">(</span>
            <span class="n">FunctionName</span><span class="o">=</span><span class="s2">"TestLambdaFunction"</span><span class="p">,</span>
            <span class="n">Runtime</span><span class="o">=</span><span class="s2">"python3.10"</span><span class="p">,</span>
            <span class="n">Role</span><span class="o">=</span><span class="n">role</span><span class="p">[</span><span class="s2">"Role"</span><span class="p">][</span><span class="s2">"Arn"</span><span class="p">],</span>
            <span class="n">Handler</span><span class="o">=</span><span class="s2">"handler.lambda_handler"</span><span class="p">,</span>
            <span class="n">Code</span><span class="o">=</span><span class="p">{</span><span class="s2">"ZipFile"</span><span class="p">:</span> <span class="n">_make_lambda_zip</span><span class="p">()},</span>
        <span class="p">)</span>

        <span class="n">response</span> <span class="o">=</span> <span class="n">lambda_client</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
            <span class="n">FunctionName</span><span class="o">=</span><span class="n">fn</span><span class="p">[</span><span class="s2">"FunctionName"</span><span class="p">],</span>
            <span class="n">Payload</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({}),</span>
        <span class="p">)</span>

        <span class="n">payload</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span>
            <span class="n">response</span><span class="p">[</span><span class="s2">"Payload"</span><span class="p">]</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">payload</span> <span class="o">==</span> <span class="p">{</span>
                <span class="s2">"statusCode"</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
                <span class="s2">"body"</span><span class="p">:</span> <span class="s2">"Hello from Lambda!"</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="o">...</span>
</pre></div>

<p>Having invoked the lambda, we can then also inspect the CloudWatch logs generated while running that function and make assertions about anything written to the log streams. In this case, I'm asserting the output of our <code>print()</code> statement made it into the logs.</p>
<div class="code"><pre class="code literal-block"><span class="k">def</span> <span class="nf">test_lambda</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">mock_aws</span><span class="p">():</span>

        <span class="o">...</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">payload</span> <span class="o">==</span> <span class="p">{</span>
                <span class="s2">"statusCode"</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
                <span class="s2">"body"</span><span class="p">:</span> <span class="s2">"Hello from Lambda!"</span>
            <span class="p">}</span>
        <span class="p">)</span>

        <span class="n">logs_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span>
            <span class="s2">"logs"</span><span class="p">,</span>
            <span class="n">region_name</span><span class="o">=</span><span class="s2">"eu-west-1"</span>
        <span class="p">)</span>
        <span class="n">log_streams</span> <span class="o">=</span> <span class="n">logs_client</span><span class="o">.</span><span class="n">describe_log_streams</span><span class="p">(</span>
            <span class="n">logGroupName</span><span class="o">=</span><span class="sa">f</span><span class="s2">"/aws/lambda/</span><span class="si">{</span><span class="n">fn</span><span class="p">[</span><span class="s1">'FunctionName'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
        <span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"logStreams"</span><span class="p">)</span>

        <span class="n">log_events</span> <span class="o">=</span> <span class="n">logs_client</span><span class="o">.</span><span class="n">get_log_events</span><span class="p">(</span>
            <span class="n">logGroupName</span><span class="o">=</span><span class="sa">f</span><span class="s2">"/aws/lambda/</span><span class="si">{</span><span class="n">fn</span><span class="p">[</span><span class="s1">'FunctionName'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
            <span class="n">logStreamName</span><span class="o">=</span><span class="n">log_streams</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"logStreamName"</span><span class="p">],</span>
        <span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"events"</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">([</span>
                <span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">log_events</span>
                <span class="k">if</span> <span class="n">e</span><span class="p">[</span><span class="s2">"message"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"log message"</span>
            <span class="p">])</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">)</span>
</pre></div>

<p>Moto really does provide an exceptionally deep and comprehensive mock AWS environment.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2024/python-package-manifest-files/" class="u-url">An analysis of python package manifest files</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2024/python-package-manifest-files/" rel="bookmark">
            <time class="published dt-published" datetime="2024-02-17T00:00:00Z" itemprop="datePublished" title="2024-02-17">2024-02-17</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Python packaging is messy and fragmented. Lots of people have been writing about it recently and there have been some great articles that have attracted a lot of attention. For example, I've particularly enjoyed:</p>
<ul>
<li>
<a href="https://alpopkes.com/posts/python/packaging_tools/">An unbiased evaluation of environment management and packaging tools</a> by Anna-Lena Popkes</li>
<li>
<a href="https://chriswarrick.com/blog/2023/01/15/how-to-improve-python-packaging/">How to improve Python packaging, or why fourteen tools are at least twelve too many</a> by Chris Warrick and</li>
<li>
<a href="https://pradyunsg.me/blog/2023/01/21/thoughts-on-python-packaging/">Thoughts on the Python packaging ecosystem</a> by Pradyun Gedam</li>
</ul>
<p>Gregory Szorc also captured the frustrating experience many developers face trying to navigate the modern python packaging landscape in <a href="https://gregoryszorc.com/blog/2023/10/30/my-user-experience-porting-off-setup.py/">My User Experience Porting Off setup.py</a>.</p>
<p>It is a topic I also spend a lot of time thinking about, but I decided to take a look at the topic from a slightly different angle. Instead of lamenting the proliferation of different tools, or attempting to round them all up and compare them, I decided to ask: What are package authors actually doing out there in the wild, and how is the community responding to this change and fragmentation?</p>
<p>So I conducted a bit of research. I looked at a sample of 31,474 public GitHub repos associated with one or more python packages on PyPI and analysed the manifests to find out a bit more about how people are actually specifying their package metadata and building their packages. For the purposes of this research, I'm focussing on packages. You could probably ask and answer some similarly interesting questions about applications, but I haven't done it here. There's a bit more information about how and why I arrived at this sample of ~30k GitHub repos in the <a href="posts/2024/python-package-manifest-files/#methodology-notes">methodology notes</a>, but I'm not going to bury the lead. Lets just jump straight into the good stuff.</p>
<h3>Manifest files</h3>
<p>I looked for the presence of 3 files: pyproject.toml, setup.py and setup.cfg. Most of the repos I looked at contained more than one.</p>
<table>
<tr>
<th>File</th>
    <th>Count</th>
    <th>Percent</th>
  </tr>
<tr>
<td>setup.py</td>
    <td class="right-align">20,684</td>
    <td class="percent" style="background-size: 66% 100%">66%</td>
  </tr>
<tr>
<td>pyproject.toml</td>
    <td class="right-align">17,245</td>
    <td class="percent" style="background-size: 55% 100%">55%</td>
  </tr>
<tr>
<td>setup.cfg</td>
    <td class="right-align">10,406</td>
    <td class="percent" style="background-size: 33% 100%">33%</td>
  </tr>
<tr>
<td><b>Total</b></td>
    <td class="right-align"><b>31,474</b></td>
    <td><b>-</b></td>
  </tr>
</table>
<h3>Pyproject.toml</h3>
<p>One of the big pushes in python is for adoption of pyproject.toml. So how is that going out there in the real world?</p>
<p>First of all, it is worth reviewing some of the ways pyproject.toml is or can be used.</p>
<ul>
<li>
<a href="https://peps.python.org/pep-0517/">PEP 517</a> Defines a way to declare a package build backend in pyproject.toml.</li>
<li>
<a href="https://peps.python.org/pep-0621/">PEP 621</a> Defines a way to define the package metadata in pyproject.toml.</li>
<li>
<a href="https://peps.python.org/pep-0518/">PEP 518</a> Defines a way to declare package build requirements in pyproject.toml and a way for python tools (which may or may not be related to packaging) to store configuration in the <code>tool.*</code> namespace. Many python tools like pytest, black, mypy, etc allow their configuration to be stored in pyproject.toml using the <code>tool.*</code> namespace.</li>
<li>In particular, poetry allows package metadata to be specified in pyproject.toml in a <code>tool.poetry</code> declaration, but predates and does not conform to PEP 621. I'm going to consider poetry separately.</li>
</ul>
<p>A point to note here is that these can be combined in various ways. For example, it is possible to declare a build backend in pyproject.toml following PEP 517 and also declare PEP 621 package metadata. However using setuptools it is also possible to declare a build backend in pyproject.toml but specify the rest of the package metadata in setup.py or setup.cfg. Some repos only use pyproject.toml for storing linter configuration and everything to do with packaging is stored in setup.py or setup.cfg. Some repos specify package metadata in pyproject.toml (either following PEP 621 or using poetry), but don't declare a build system. One does not necessarily imply another. I found examples of pretty much every combination. This makes it difficult to conduct a completely coherent analysis or arrive at universally valid assumptions.</p>
<p>In the sample of repos I looked at 17,245 (55%) contained a pyproject.toml file. 15,754 (91%) of those declare a build backend, requirements, and/or package metadata. 1,497 (9%) did not contain any of those things. Presumably in basically all of those cases, pyproject.toml is being used exclusively as a configuration file for dev tooling.</p>
<table>
<tr>
<th>Feature</th>
    <th>Count</th>
    <th>Percent</th>
  </tr>
<tr>
<td>Has build requirements</td>
    <td class="right-align">15,427</td>
    <td class="percent" style="background-size: 89% 100%">89%</td>
  </tr>
<tr>
<td>Has build backend</td>
    <td class="right-align">14,328</td>
    <td class="percent" style="background-size: 83% 100%">83%</td>
  </tr>
<tr>
<td>Has PEP 621 metadata</td>
    <td class="right-align">6,563</td>
    <td class="percent" style="background-size: 38% 100%">38%</td>
  </tr>
<tr>
<td>Has Poetry metadata</td>
    <td class="right-align">4,890</td>
    <td class="percent" style="background-size: 28% 100%">28%</td>
  </tr>
<tr>
<td>Has no packaging metadata</td>
    <td class="right-align">1,497</td>
    <td class="percent" style="background-size: 9% 100%">9%</td>
  </tr>
<tr>
<td><b>Total</b></td>
    <td class="right-align"><b>17,245</b></td>
    <td><b>-</b></td>
  </tr>
</table>
<p>There are a few interesting results here. The first is that most repos containing a pyproject.toml declare either a build backend and/or requirements. I was actually surprised that more files declare build requirements than a build backend. I expected repos declaring build requirements would basically be a subset of those declaring a build backend. Turns out the inverse is true.</p>
<p>Many repos are declaring package metadata in pyproject.toml using either PEP 621 or Poetry format, but adoption of pyproject.toml for this purpose is less common.</p>
<p>My hunch is that a lot of the repos which are only specifying build backend/requirements may have adopted pyproject.toml primarily as a configuration format (as opposed to a package manifest format) and then added a minimal <code>build-system</code> declaration for compatibility purposes. However that is just my conjecture.</p>
<h3>Setup.py and setup.cfg</h3>
<p>The oldest way to specify package metadata is using setup.py. This has served the community well for many years, but the package metadata is mixed with executable python code. The python community's first attempt at a declarative manifest format was setup.cfg. This was a format specific to setuptools rather than a standard and the setuptools project plans to <a href="https://github.com/pypa/setuptools/issues/3214">eventually deprecate</a> setup.cfg. One of the big pushes in python is for moving away from setup.py and setup.cfg to specify package metadata, and towards pyproject.toml. So how is that going out there in the real world?</p>
<p>Of the repos I looked at, 20,684 (66%) contained a setup.py and 10,406 (33%) contained a setup.cfg file. Many contained both. As with pyproject.toml, presence or absence of the file in a repo doesn't necessarily tell us the full story. Some repos that are primarily using pyproject.toml also have a stub setup.py that just contains</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">setuptools</span>
<span class="n">setuptools</span><span class="o">.</span><span class="n">setup</span><span class="p">()</span>
</pre></div>

<p>for backwards compatibility reasons. This may be needed, for example for compatibility with tools that don't support <a href="https://peps.python.org/pep-0660/">PEP 660</a> editable installs.</p>
<p>As with pyproject.toml, many python based tools like isort and flake8 allow their configuration to be stored in setup.cfg so some repos contain a setup.cfg but aren't using to store any information related to packaging - it is just there to store linter configuration. Again, basically every combination of scenarios exists in the sample of repos I looked at.</p>
<p>I haven't attempted to parse the setup.py and setup.cfg files. I am perhaps missing a bit of nuance here, but I have made some assumptions:</p>
<ul>
<li>A repo which declares poetry or PEP 621 package metadata in pyproject.toml is using pyproject.toml as the package manifest.</li>
<li>A repo that has a setup.py but not a setup.cfg and either doesn't have a pyproject.toml at all or has a pyproject.toml which does not contain poetry or PEP 621 package metadata is using setup.py as the package manifest.</li>
<li>A repo that has a setup.py and setup.cfg and either doesn't have a pyproject.toml at all or has a pyproject.toml which does not contain poetry or PEP 621 package metadata is using setup.py or setup.cfg as the package manifest.</li>
<li>There were also just over 1,000 repos doing some other combination of things. A lot of these were a pyproject.toml declaring a build system or build requirements only, with metadata in setup.cfg. I didn't attempt to break them down any further.</li>
</ul>
<table>
<tr>
<th>Manifest type</th>
    <th>Count</th>
    <th>Percent</th>
  </tr>
<tr>
<td>pyproject.toml with metadata</td>
    <td class="right-align">11,349</td>
    <td class="percent" style="background-size: 36% 100%">36%</td>
  </tr>
<tr>
<td>setup.py only</td>
    <td class="right-align">10,695</td>
    <td class="percent" style="background-size: 34% 100%">34%</td>
  </tr>
<tr>
<td>setup.py and setup.cfg</td>
    <td class="right-align">8,235</td>
    <td class="percent" style="background-size: 26% 100%">26%</td>
  </tr>
<tr>
<td>Other</td>
    <td class="right-align">1,195</td>
    <td class="percent" style="background-size: 4% 100%">4%</td>
  </tr>
<tr>
<td><b>Total</b></td>
    <td class="right-align"><b>31,474</b></td>
    <td class="percent" style="background-size: 100% 100%"><b>100%</b></td>
  </tr>
</table>
<p>18,930 (63%) of the repos I looked at are sticking with setup.py and/or setup.cfg as the package manifest.</p>
<p>Using only a setup.py is still a very popular method of packaging at 34%. This is nearly equal with storing package metadata in pyproject.toml at 36%, despite efforts to transition the community away from executable package manifests and towards declarative manifest formats.</p>
<h3>Build Backends</h3>
<p>14,328 of the repos I looked at are using a pyproject.toml that declares a PEP-517 build backend. So next I dug into that. Which build backends are these repos using?</p>
<table>
<tr>
<th>Build backend</th>
    <th>Count</th>
    <th>Percent</th>
  </tr>
<tr>
<td>Setuptools</td>
    <td class="right-align">6,732</td>
    <td class="percent" style="background-size: 47% 100%">47%</td>
  </tr>
<tr>
<td>Poetry</td>
    <td class="right-align">4,671</td>
    <td class="percent" style="background-size: 33% 100%">33%</td>
  </tr>
<tr>
<td>Hatch</td>
    <td class="right-align">1,592</td>
    <td class="percent" style="background-size: 11% 100%">11%</td>
  </tr>
<tr>
<td>Flit</td>
    <td class="right-align">687</td>
    <td class="percent" style="background-size: 5% 100%">5%</td>
  </tr>
<tr>
<td>Other</td>
    <td class="right-align">223</td>
    <td class="percent" style="background-size: 2% 100%">2%</td>
  </tr>
<tr>
<td>Pdm</td>
    <td class="right-align">215</td>
    <td class="percent" style="background-size: 2% 100%">2%</td>
  </tr>
<tr>
<td>Maturin</td>
    <td class="right-align">208</td>
    <td class="percent" style="background-size: 1% 100%">1%</td>
  </tr>
<tr>
<td><b>Total</b></td>
    <td class="right-align"><b>14,328</b></td>
    <td class="percent" style="background-size: 100% 100%"><b>100%</b></td>
  </tr>
</table>
<p>There are more interesting findings here:</p>
<ul>
<li>Among repos using pyproject.toml, setuptools is the by far the most commonly declared build backend, accounting for nearly half the repos I looked at.</li>
<li>New shiny tools like poetry, hatch and flit have some adoption, but account for a much smaller share of the ecosystem.</li>
<li>By far the most widely used of these more modern packaging tools is poetry, accounting for 33% of the repos I looked at declaring a build backend in pyproject.toml.</li>
</ul>
<h3>Setuptools</h3>
<p>Finally, I wanted to look at those repos using setuptools and pyproject.toml. Broadly, these are going to divide into 2 camps:</p>
<ul>
<li>Those specifying package metadata in pyproject.toml, following PEP-621</li>
<li>Those specifying a build backend only in pyproject.toml, following PEP-517, but storing the package metadata in setup.py or setup.cfg.</li>
</ul>
<table>
<tr>
<th>Metadata location</th>
    <th>Count</th>
    <th>Percent</th>
  </tr>
<tr>
<td>Outside pyproject.toml</td>
    <td class="right-align">3,615</td>
    <td class="percent" style="background-size: 54% 100%">54%</td>
  </tr>
<tr>
<td>Inside pyproject.toml (PEP-621)</td>
    <td class="right-align">3,117</td>
    <td class="percent" style="background-size: 46% 100%">46%</td>
  </tr>
<tr>
<td><b>Total</b></td>
    <td class="right-align"><b>6,732</b></td>
    <td class="percent" style="background-size: 100% 100%"><b>100%</b></td>
  </tr>
</table>
<p>Among repos using setuptools and pyproject.toml, only a minority have adopted PEP-621 for declaring package metadata. In the sample of repos I looked at which declare setuptools as a build backend in pyproject.toml, the most popular approach (albeit by a small margin) is to declare only the build backend details in pyproject.toml and store the package metadata elsewhere.</p>
<h3>Conclusions</h3>
<p>Based on the analysis I've done here, it seems reasonable to say that adoption of pyproject.toml has been slow, particularly as a package manifest format. Most of the repos I looked at are only or primarily using setup.py and/or setup.cfg. Modern packaging tools are generating blog posts, debate, and mindshare. Out there in the real world we are seeing limited adoption in comparison to more traditional approaches. While a blog post about setuptools is less likely to hit the front page of hackernews, setuptools is the real workhorse when it comes to getting packages shipped.</p>
<p>As noted at the start of this article, python packaging is a confusing and fragmented space at the moment. There are a lot of ways to skin this cat. It seems reasonable to infer that as a response to this, many developers are choosing to stick with an existing working solution, rather than make sense of the chaos. Who can blame them?</p>
<p>The python community often moves slowly in response to change. For example the migration from python 2 to 3 dragged on for about a decade, but in that case the direction of travel was at least clear. There was a single linear path. When it comes to modernizing the packaging space, progress is also hindered by the fact that for some projects there are many possible directions of travel. For some projects, there are still zero. Perhaps this is a journey that will take even longer to shake out.</p>
<h3 id="methodology-notes">Methodology notes</h3>

<p>This research was based on a convenience sample. I looked at a selection of repos that made it quick and easy to harvest data, rather than the most robust sample or a complete census of PyPI.</p>
<p>As a starting point, I used the 2023-10-22 <a href="https://packages.ecosyste.ms/open-data">Ecosyste.ms Open Data</a> Release (which is licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> ). This was an easy place to get a bulk list of python packages with GitHub repos attached. I then applied a few filters.</p>
<p>First I excluded any packages which didn't have one or more releases published inside 2023. I'm really looking into modern packaging practices, so packages without a recent release are less useful to consider here.</p>
<p>Then I excluded any packages that had less than 100 downloads in the last month. There is a lot of junk on PyPI. This is a low bar for popularity, but I wanted to apply some kind of measure of "someone is actually using this code for something". Applying even this modest filter excluded a surprisingly large number of packages.</p>
<p>Then finally, I looked only at packages which had a GitHub repo attached to them in the Ecosyste.ms data. This was mainly about making it easy to fetch data in bulk. This means I excluded repos hosted on GitLab, BitBucket, CodeBerg, etc from this analysis. I also did not attempt to look at packages that had no <code>repository_url</code> attached in the data. As such, the sample contains some blind spots.</p>
<p>After de-duplicating, this gave me 35,732 GitHub repository URLs.</p>
<p>I then used the GitHub GraphQL API to attempt to fetch a setup.py, setup.cfg and pyproject.toml if they existed in the repo root. After excluding any repos that were private, did not exist at all, or repos that didn't contain any of those files in the root, I was left with the 31,474 repos that formed the basis of this analysis. Another obvious blind spot here is repos that host a package in a subdirectory instead of the repo root. Those will have been excluded too.</p>
<p>Finally, I grabbed whatever files were at the <code>HEAD</code> of the default branch in GitHub. I didn't attempt to find a latest release, or the release that would have been current at the time of the ecosyste.ms open data release. I don't think this makes a huge difference, but it is worth noting.</p>
<h3>Future work</h3>
<p>This has been an interesting process, but it only represents a snapshot in a landscape that is shifting over time. I'd like to repeat this analysis again in future to see how the things have changed. It's been a blast. Let's do it again some time.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2024/arq-taskiq/" class="u-url">Arq and TaskIQ</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2024/arq-taskiq/" rel="bookmark">
            <time class="published dt-published" datetime="2024-01-18T00:00:00Z" itemprop="datePublished" title="2024-01-18">2024-01-18</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>At work, we recently found ourselves in the market for an asynchronous task queue for python. A traditional task queue like Celery can be said to be "asynchronous" in the sense that your web server can kick a task into the queue and continue processing the web request without waiting for the task to complete. However it is "synchronous" in the sense that the task functions in your queue must be synchronous functions (declared with <code>def</code> rather than <code>async def</code>). If you want to queue an <code>async</code> function, you need an async worker to process it.</p>
<p>The two contenders we've been looking at in this space are <a href="https://arq-docs.helpmanual.io/">arq</a> and <a href="https://taskiq-python.github.io/">taskiq</a>. These two solutions take slightly different approaches to solving the same problem.</p>
<p>Taskiq takes a conceptually simple push/pop approach to interacting with the queue. This is the same model used by popular synchronous packages like Celery and rq. When a worker is free to take a task, it pops a task off the queue and then executes it. The downside of this approach is that if a worker pops a task off the queue and then shuts down without processing the task to completion, that task is already gone from your queue without having been run to completion. Another worker can't try it again.</p>
<p>Arq takes a different approach called "pessimistic execution" which solves that specific problem. When an arq worker takes a task from the queue, it doesn't remove it from the queue yet. The task stays in the queue while it is being run. The task is finally deleted from the queue in a post hook after the task is complete. This means a task is only removed from the queue after it has run to completion.</p>
<p>In order to ensure every worker in your cluster is not trying to process the same task at once, arq also maintains some additional shared state. When a worker takes a task, the worker acquires a lock on a task. That lock is automatically set to release after a timeout. If a worker never deletes the task in the post hook, the task is eventually unlocked and becomes available for another worker to process at a later time once the timeout expires.</p>
<p>This gives arq some slightly different characteristics than taskiq.</p>
<p>Arq will ideally try to deliver your task exactly once, but guarantees "at least once execution". Executing your task multiple times is considered preferable to executing it zero times. This means no lost tasks, but it also means if you use arq, your tasks must be written to be idempotent.</p>
<p>The simple push/pop relationship with the queue employed by taskiq lends itself to being compatible with a wide range of backends. Taskiq already has plugins for using NATS, Redis, RabbitMQ and Kafka as brokers. Taskiq defines a <a href="https://taskiq-python.github.io/extending-taskiq/broker.html">plugin interface</a> for brokers, so it would be possible to write plugins for aditional backends like SQS for example.</p>
<p>Conversely, arq has a more complicated relationship with the data store. The additional shared state required to implement the locking behaviour needs a richer set of operations. As such, arq is tightly coupled to redis as a backend. There is no mechanism to substitute another broker.</p>
<p>So here's a summary of those tradeoffs:</p>
<ul>
<li>Arq is tied to redis. It provides stronger guarantees about eventual task execution and requires you to write your tasks with the assumption they could be attempted multiple times.</li>
<li>Taskiq follows a model similar to Celery or rq. It provides weaker assurances, but this conceptually simpler model means you can assume your tasks will only be executed once. This setup also allows for compatibility with a wider range of brokers.</li>
</ul>
<p>Our project has a lot of long-running tasks, which are vulnerable to being killed off before running to completion by deploy or scale-in events. Because of this, we prefer the pessimistic execution model offered by arq. We ended up moving forward with arq for our project.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/2023/side-project/" class="u-url">So, you want to start a side project?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                chris48s
            </span></p>
            <p class="dateline">
            <a href="posts/2023/side-project/" rel="bookmark">
            <time class="published dt-published" datetime="2023-12-03T00:00:00Z" itemprop="datePublished" title="2023-12-03">2023-12-03</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Over the years, I have started or worked on many side projects and spent a lot of time maintaining them. This has taught me a lot about what makes a project easy and hard to maintain. This post is a reflection on some of the lessons learned over that time.</p>
<p>First, lets start off with some assumptions:</p>
<ol>
<li>You want to start a side project, not a side hustle. A side hustle is an entrepreneurial exercise. It is something you eventually want to turn into a business or job, even if it is not on day one. The objective of a side project is the creative satisfaction of the project itself. It might be a learning experience, or exploration of personal interests.</li>
<li>Your side project is software or programming related.</li>
<li>The project is in some way public. It might be open source. You want to make something with a userbase or community beyond just yourself.</li>
<li>Crucially, you care about maintaining this project over a period of time. It is not just a throwaway learning exercise.</li>
</ol>
<p>If you are looking to start a side project, this implies you have some time on your hands right now. This is a great place to be, but it won't be true forever. Life happens, and it usually happens unexpectedly.</p>
<p>So, how can we optimise for projects that are low-maintenance or at least projects that require the type of maintenance that can be done on our own terms? A side project that may require attention urgently and unexpectedly can quickly become a burden.</p>
<h3>Third party APIs</h3>
<p>Third Party APIs are one of the most likely sources of sudden and unexpected maintenance tasks. You may or may not get warning when changes happen. If you use an API with authentication or credentials, you probably had to sign up for an account so the upstream service provider probably does at least hold some contact details they could use to inform you of changes. Expect the unexpected from any API where you use public or anonymous access.</p>
<p>Sometimes an API you depend on will:</p>
<ul>
<li>Make a non-backwards compatible change.</li>
<li>Introduce rate limits or enforce stricter rate limits.</li>
<li>Change their terms of service such that your project now violates them in some way. </li>
<li>Withdraw service completely or shut down.</li>
</ul>
<p>All of this is very in vogue at the moment.</p>
<h3>Scrapers</h3>
<p>Web scrapers are like third party APIs, only worse. API authors expect that other people's code depends on their API. They may still choose to make a breaking change anyway, but there is some incentive there to maintain a stable platform for their users. Nobody assumes or cares that your code relies on scraping their website. It certainly won't be a consideration in changing it. You definitely won't get any communication informing you of a change that impacts your code. Website authors may even be actively trying to prevent you from scraping them. Code that relies on web scraping is certain to break at some point. It is matter of <em>when</em>, rather than <em>if</em>.</p>
<h3>Infrastructure</h3>
<p>Any kind of infrastructure you run (web servers, DB, cache, etc) is going to come with some maintenance overhead. Applying security upgrades, backup and restore, ensuring uptime, etc. The exact tasks that come up will vary a bit depending on the type of infrastructure, but could include a mix of tasks that can be planned in advance and things that happen unexpectedly. You can plan or defer applying an upgrade, but data loss requiring a restore from backup will happen when you least expect it.</p>
<p>There are some tradeoffs to be made here. Using a managed service can outsource some of this maintenance. For example, if we consider something like a Postgres DB: Running your own Postgres instance on a VPS leaves everything up to you (of course, with a side project, managing this yourself could be part of the joy or satisfaction). A fully managed service like Heroku Postgres will handle most of this stuff for you transparently. Something like RDS or Fly.io's "semi-managed" Postgres sits somewhere in the middle of those two extremes.</p>
<p>A fully managed service comes with some costs though. A managed service can be it's own source of breaking changes or deprecations. Some platforms have a greater or lesser reputation for stability (think AWS vs GCP ðŸ˜€). The more obvious cost is the literal financial cost though, which brings us on to..</p>
<h3>Finances</h3>
<p>If you go down the route of a project that requires some sort of infrastructure, that has a cost associated with it, and somebody needs to cover that. Maybe you will pay for it out of your own pocket. In general, side projects are not revenue generating and need to run on a modest budget. Often that budget will be zero. Maybe you can run a service using free tier offerings. However, even if you're using it for free, someone is still paying. For the moment, the company offering that "free" service is covering that cost out of marketing budget because offering a free tier is good promotion, but that might not stay true forever. If your project is popular, you might also consider soliciting some sponsorship to cover your costs, either from your community or a corporate sponsor.</p>
<p>Again, funding concerns are another common source of suddenly urgent work.</p>
<ul>
<li>Your project may become more popular, outgrowing your current pricing plan or the level of sponsorship your project currently attracts.</li>
<li>If your project runs on a free tier service, that offering will probably be withdrawn at some point. Most of them are, eventually.</li>
<li>If you have a corporate sponsor, also assume it will not last forever. Sponsorships of open source and community projects are often the first things to be cut when times are tough.</li>
</ul>
<p>All of this can leave you quickly scrambling to migrate to another service, find ways to consume fewer resources, or present an immediate existential threat to your project.</p>
<h3>Personal data</h3>
<p>nopenopenopenopenope</p>
<p>If your side project stores personal data, congratulations. You now have compliance obligations. Choo choo ðŸš‚ All aboard the fun train! A service that stores any kind of personal data (e.g: user accounts) is not a good choice for a side project. This one is a hard no from me.</p>
<h3>It's not all doom and gloom</h3>
<p>That is a list of things that can, to one degree or another, generate some maintenance overhead. So what are some types of projects that don't have any of those characteristics (or at least as few of them as possible)?</p>
<h3>Command line applications (compiled language)</h3>
<p>If your project is distributed as a compiled binary and it doesn't call any external APIs, there are very few externalities that can break this type of project or require attention from you as a maintainer. This is even more true for a statically linked binary. The only real exception to this might be needing to respond to a security issue.</p>
<h3>Command line applications (dynamic language) or Libraries</h3>
<p>This type of project has similar properties to a compiled command line tool. However, with anything that the user installs via <code>pip install</code>, <code>npm install</code>, etc, your dependencies are resolved at install time. This means your previously working code can be broken for some users by non-backwards compatible changes made in an in-range dependency version. In theory SemVer saves us here, but in most languages (other than javascript) it is necessary to support wide ranges. This type of breakage is not super common, but it does happen.</p>
<h3>Static sites</h3>
<p>Static content is good content. If you have the type of static site that can be served from a S3 bucket, there are multiple places that will host it for free and scale it to handle as much traffic as the internet can throw at it. If you do need to move it, it is relatively easy and you have zero infrastructure to maintain.</p>
<p>For a low-maintenance project side project that involves a website, "could this be a static site?" is generally a good question to ask. Sometimes by making a compromise or two, it is possible to get rid of a web server and DB and replace them with a static site. This is usually an advisable tradeoff. A good example of this might be choosing a SSG for your blog, instead of hosting a CMS. (<strong>Edit:</strong> A couple of months after I wrote this I came across this article <a href="https://newsletter.pnote.eu/p/simple-lasts-longer">simple lasts longer</a> by Przemek, which gives a great concrete example of making some tradeoffs to allow a project to be delivered as a static site in preference to running a database).</p>
<p>It is worth noting that this is not true of the type of "static site" which is heavily tied to the specific features of a platform like Vercel or Netlify. These basically have the same tradeoffs as managed infrastructure with the additional downside of vendor lock-in.</p>
<h3>End</h3>
<p>So, that's some thoughts on the characteristics of a low-maintenance side project. Go forth. May your side project bring you many hours of joy and few unexpected urgent maintenance issues.</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-6.html" rel="next">Older posts</a>
            </li>
        </ul></nav></main><footer id="footer"><p>
</p>
<p>Contents Â© 2026 chris48s</p>

<ul class="no-bullet">
<li>
    <i class="fab fa-github"></i>
    <a href="https://github.com/chris48s/" target="_blank">Github</a>
  </li>
  <li>
    <i class="fab fa-mastodon"></i>
    <a href="https://fed.chris-shaw.dev/@chris" target="_blank">Mastodon</a>
  </li>
</ul></footer>
</div>
    
            <script src="assets/js/baguetteBox.min.js"></script><script>
    baguetteBox.run('main#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
